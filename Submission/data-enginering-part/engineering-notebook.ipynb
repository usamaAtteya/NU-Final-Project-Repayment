{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding xgboost jars for pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hdfs\"\n",
    "os.environ[\"PYTHON_VERSION\"] = \"3.5.2\"\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_app_model_path = \"/Users/apple/Desktop/flask_projects/repayment-api/final_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoderEstimator,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.master', 'local[2]') \\\n",
    "    .appName('Final-Project') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#     .config(\"spark.jars.packages\",\"spark-avro_2.12-3.0.0-preview2\")\\\n",
    "\n",
    "#  .config(\"spark.jars\", \"xgboost4j-0.90.jar,xgboost4j-spark-0.90.jar\")\\\n",
    "#     .config(\"spark.driver.extraClassPath\", \"xgboost4j-0.90.jar,xgboost4j-spark-0.90.jar\")\\\n",
    "#     .config(\"spark.executor.extraClassPath\", \"xgboost4j-0.90.jar,xgboost4j-spark-0.90.jar\")\\\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding PySpark wrapper to xgboost-scala jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_data =  spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/Users/apple/Desktop/BD2/bd2/data/CollegeScorecard_Raw_Data/MERGED2012_13_PP.csv\")\\\n",
    "\n",
    "# scorecard_data_rdd =  spark.read.text(paths= \"hdfs://localhost:9000/repayment/scorecard\")\n",
    "# spark.read\\\n",
    "#   .load(\"hdfs://localhost:9000/repayment/scorecard_data.csv\")\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =[\"CONTROL\",\"ADM_RATE\",\"ADM_RATE_ALL\",\"SAT_AVG_ALL\",\"SATMTMID\",\"UGDS\",\"HIGHDEG\",  \"TUITFTE\", \n",
    "       \"COSTT4_A\", \"PCTFLOAN\",\"COMP_ORIG_YR2_RT\", \"UGDS_WHITE\",\"UGDS_BLACK\",\"UGDS_HISP\",\"UGDS_ASIAN\",\"UGDS_AIAN\",\"UGDS_NHPI\",\"UGDS_2MOR\",\"UGDS_NRA\",\"UGDS_UNKN\",\"PPTUG_EF\",\"COSTT4_P\",\"TUITIONFEE_IN\",\"TUITIONFEE_OUT\",\"TUITIONFEE_PROG\",\"INEXPFTE\",\"PCTPELL\",\"COMP_ORIG_YR3_RT\",\"LOAN_COMP_ORIG_YR3_RT\",\"DEATH_YR4_RT\",\"COMP_ORIG_YR4_RT\",\"AGE_ENTRY\",\"COUNT_NWNE_P10\",\"COUNT_WNE_P10\",\"MN_EARN_WNE_P10\",\"MD_EARN_WNE_P10\",\"COMPL_RPY_1YR_RT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features = [\"CONTROL\",\"HIGHDEG\"]\n",
    "target = \"COMPL_RPY_1YR_RT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this preprocessing step is important only in training, because in run time we will not receive any value as privacySuppressed, but it will be null\n",
    "def cleanPrivacySuppressed(dataFrame):\n",
    "    # define udf to clean PrivacySuppressed for dataframe  column\n",
    "    clean_privacy_suppressed =F.udf(lambda value: np.nan if value == \"PrivacySuppressed\" else value)\n",
    "    # apply the function on every column in dataframe\n",
    "    dataFrame = dataFrame.select([ clean_privacy_suppressed(c).cast(\"double\" if c not in categ_features else \"int\")  for c in dataFrame.columns if c in features])\n",
    "    # restore old names of columns (previously applied function changed col names)\n",
    "    for index in range(len(dataFrame.columns)):\n",
    "        dataFrame = dataFrame.withColumnRenamed(dataFrame.columns[index], dataFrame.columns[index].replace(\"CAST(<lambda>(\",\"\").replace(\") AS DOUBLE)\",\"\").replace(\") AS INT)\",\"\"))     \n",
    "    return dataFrame\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this preprocessing step is important only in training, so it will not be included in the pipeline.\n",
    "scorecard_data_cleaned = cleanPrivacySuppressed(scorecard_data)\n",
    "# select not null target rows\n",
    "scorecard_data_cleaned =scorecard_data_cleaned.where(F.col(target).isNotNull()).where(F.col(target)!= np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot encoder preprocessing\n",
    "hot_encoder = OneHotEncoderEstimator(inputCols=categ_features, outputCols=[\"{0}_ENCODED\".format(colName) for colName in categ_features] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector assembler\n",
    "model_input_features = [f for f in features if f not in categ_features and f is not target ]\n",
    "model_input_features.extend([\"{0}_ENCODED\".format(colName) for colName in categ_features])\n",
    "vec_assembler = VectorAssembler(inputCols=model_input_features,outputCol=\"features\",handleInvalid=\"keep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[HIGHDEG: int, CONTROL: int, ADM_RATE: double, ADM_RATE_ALL: double, SATMTMID: double, SAT_AVG_ALL: double, UGDS: double, UGDS_WHITE: double, UGDS_BLACK: double, UGDS_HISP: double, UGDS_ASIAN: double, UGDS_AIAN: double, UGDS_NHPI: double, UGDS_2MOR: double, UGDS_NRA: double, UGDS_UNKN: double, PPTUG_EF: double, COSTT4_A: double, COSTT4_P: double, TUITIONFEE_IN: double, TUITIONFEE_OUT: double, TUITIONFEE_PROG: double, TUITFTE: double, INEXPFTE: double, PCTPELL: double, PCTFLOAN: double, COMP_ORIG_YR2_RT: double, COMP_ORIG_YR3_RT: double, LOAN_COMP_ORIG_YR3_RT: double, DEATH_YR4_RT: double, COMP_ORIG_YR4_RT: double, COMPL_RPY_1YR_RT: double, AGE_ENTRY: double, COUNT_NWNE_P10: double, COUNT_WNE_P10: double, MN_EARN_WNE_P10: double, MD_EARN_WNE_P10: double, CONTROL_ENCODED: vector, HIGHDEG_ENCODED: vector, features: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing pipleline\n",
    "preprocessing_pipeline = Pipeline(stages=[hot_encoder,vec_assembler])\n",
    "preprocessed_data = preprocessing_pipeline.fit(scorecard_data_cleaned).transform(scorecard_data_cleaned)\n",
    "# cache this preprocessing step, this is performance optimization step for model-tunning process\n",
    "preprocessed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[HIGHDEG: int, CONTROL: int, ADM_RATE: double, ADM_RATE_ALL: double, SATMTMID: double, SAT_AVG_ALL: double, UGDS: double, UGDS_WHITE: double, UGDS_BLACK: double, UGDS_HISP: double, UGDS_ASIAN: double, UGDS_AIAN: double, UGDS_NHPI: double, UGDS_2MOR: double, UGDS_NRA: double, UGDS_UNKN: double, PPTUG_EF: double, COSTT4_A: double, COSTT4_P: double, TUITIONFEE_IN: double, TUITIONFEE_OUT: double, TUITIONFEE_PROG: double, TUITFTE: double, INEXPFTE: double, PCTPELL: double, PCTFLOAN: double, COMP_ORIG_YR2_RT: double, COMP_ORIG_YR3_RT: double, LOAN_COMP_ORIG_YR3_RT: double, DEATH_YR4_RT: double, COMP_ORIG_YR4_RT: double, COMPL_RPY_1YR_RT: double, AGE_ENTRY: double, COUNT_NWNE_P10: double, COUNT_WNE_P10: double, MN_EARN_WNE_P10: double, MD_EARN_WNE_P10: double, CONTROL_ENCODED: vector, HIGHDEG_ENCODED: vector, features: vector]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data to train/test 80/20\n",
    "train_preprocessed_data = preprocessed_data.randomSplit([.8,.2])[0]\n",
    "train_preprocessed_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "gbmodel = GBTRegressor(featuresCol=\"features\",labelCol=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model tuning process\n",
    "evaluator =RegressionEvaluator(labelCol=target)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbmodel.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbmodel.maxBins, [20, 60])\n",
    "             .addGrid(gbmodel.maxIter, [10, 20])\n",
    "             .addGrid(gbmodel.minInfoGain, [0.0, 0.05])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=gbmodel, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "\n",
    "pipeline_cv = cv.fit(train_preprocessed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final pipeline to deploy is the preprocessing steps + best model (best hyperparameters)\n",
    "final_pipeline = Pipeline(stages=[*preprocessing_pipeline.getStages(), pipeline_cv.bestModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on all data and save model to disk\n",
    "final_model = final_pipeline.fit(scorecard_data_cleaned)\n",
    "final_model.write().overwrite().save(web_app_model_path)"
   ]
  }
 ],
 "metadata": {
  "customDeps": [
   "ml.dmlc:xgboost4j-spark:0.9:SNAPSHOT"
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
